{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47530785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\garim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Download Spanish stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e51fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "def remove_stopwords(text, stop_words= spanish_stopwords):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    doc = nlp(' '.join(words))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def preprocess_text(text, remove_stopwords= False, remove_stopwords_func= remove_stopwords):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        text = remove_stopwords_func(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def filter_tokens(tokens, stop_words= spanish_stopwords):\n",
    "    return [tok for tok in tokens if ('_' in tok or tok not in stop_words)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b50a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola visitas para más información cuántos años tienes tengo años\n",
      "hola visitas información cuántos años años\n",
      "holar visita información cuántos año año\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "spanish_text = \"¡Hola! Visitas https://example.com para más información. ¿Cuántos años tienes? Tengo 25 años.\"\n",
    "cleaned_text = preprocess_text(spanish_text, remove_stopwords= False)\n",
    "print(cleaned_text)\n",
    "\n",
    "cleaned_text = remove_stopwords(cleaned_text)\n",
    "print(cleaned_text)\n",
    "\n",
    "cleaned_text = \" \".join(lemmatize_words(cleaned_text.split()))\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042d2f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>url</th>\n",
       "      <th>ingredientes</th>\n",
       "      <th>pasos</th>\n",
       "      <th>pais</th>\n",
       "      <th>duracion</th>\n",
       "      <th>porciones</th>\n",
       "      <th>calorias</th>\n",
       "      <th>categoria</th>\n",
       "      <th>contexto</th>\n",
       "      <th>comensales</th>\n",
       "      <th>tiempo</th>\n",
       "      <th>dificultad</th>\n",
       "      <th>categoria 2</th>\n",
       "      <th>valoracion</th>\n",
       "      <th>votos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>berenjenas rellenas</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>2 berenjenas, 1 pimiento rojo, 1 pimiento amar...</td>\n",
       "      <td>paso 1. lava bien las berenjenas y pártelas po...</td>\n",
       "      <td>españa</td>\n",
       "      <td>45 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alto en fibra, sin grasas trans, sin sodio o s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcachofas al horno con pico de gallo</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>4 alcachofas, 1 limón, 400 g de carne picada (...</td>\n",
       "      <td>paso 1. precalentar el horno a 180?°c. , paso ...</td>\n",
       "      <td>españa</td>\n",
       "      <td>60 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, sin grasa, alto en fibra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arroz basmati salteado con heura y verduras va...</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>200 g de arroz basmati integral, 450 ml de agu...</td>\n",
       "      <td>paso 1. hervir el arroz basmati durante unos 2...</td>\n",
       "      <td>españa</td>\n",
       "      <td>40 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, alto en grasas, bueno fuente...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tataki de atún</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>200 gramos de atún rojo (fresco o congelado), ...</td>\n",
       "      <td>paso 1. cortar en forma rectangular para el ta...</td>\n",
       "      <td>españa</td>\n",
       "      <td>60 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, sin grasa, alto en fibra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>merluza al vapor a la gallega</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>700 g de merluza en rodajas, 3 ajos, 1 manojo ...</td>\n",
       "      <td>paso 1. pelar y lavar las patatas. retirar el ...</td>\n",
       "      <td>españa</td>\n",
       "      <td>45 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, sin grasa, alto en fibra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nombre  \\\n",
       "0                                berenjenas rellenas   \n",
       "1              alcachofas al horno con pico de gallo   \n",
       "2  arroz basmati salteado con heura y verduras va...   \n",
       "3                                     tataki de atún   \n",
       "4                      merluza al vapor a la gallega   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "1  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "2  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "3  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "4  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "\n",
       "                                        ingredientes  \\\n",
       "0  2 berenjenas, 1 pimiento rojo, 1 pimiento amar...   \n",
       "1  4 alcachofas, 1 limón, 400 g de carne picada (...   \n",
       "2  200 g de arroz basmati integral, 450 ml de agu...   \n",
       "3  200 gramos de atún rojo (fresco o congelado), ...   \n",
       "4  700 g de merluza en rodajas, 3 ajos, 1 manojo ...   \n",
       "\n",
       "                                               pasos    pais duracion  \\\n",
       "0  paso 1. lava bien las berenjenas y pártelas po...  españa   45 min   \n",
       "1  paso 1. precalentar el horno a 180?°c. , paso ...  españa   60 min   \n",
       "2  paso 1. hervir el arroz basmati durante unos 2...  españa   40 min   \n",
       "3  paso 1. cortar en forma rectangular para el ta...  españa   60 min   \n",
       "4  paso 1. pelar y lavar las patatas. retirar el ...  españa   45 min   \n",
       "\n",
       "  porciones  calorias categoria contexto  comensales tiempo  dificultad  \\\n",
       "0         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "1         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "2         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "3         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "4         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "\n",
       "                                         categoria 2  valoracion  votos  \n",
       "0  alto en fibra, sin grasas trans, sin sodio o s...         NaN    NaN  \n",
       "1         bajo en calorías, sin grasa, alto en fibra         NaN    NaN  \n",
       "2  bajo en calorías, alto en grasas, bueno fuente...         NaN    NaN  \n",
       "3         bajo en calorías, sin grasa, alto en fibra         NaN    NaN  \n",
       "4         bajo en calorías, sin grasa, alto en fibra         NaN    NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/recetas_limpias.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb0321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the 'pasos' column. Dont remove stopwords\n",
    "train_data = df['pasos'].dropna().apply(preprocess_text).str.split()\n",
    "train_data = pd.concat([train_data, df['contexto'].dropna().apply(preprocess_text).str.split()], axis=0) # add context sentences\n",
    "train_data = pd.concat([train_data, df['ingredientes'].dropna().apply(preprocess_text).str.split()], axis=0) # add inhredient list\n",
    "\n",
    "# Detect frequent bigrams and trigrams. Bigrams should appear in atleast 100 recipes\n",
    "bigram = Phrases(train_data, min_count= 100, threshold= 20)\n",
    "trigram = Phrases(bigram[train_data], min_count= 25, threshold= 5)\n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Step 4: Apply the phrasers to get merged n-grams\n",
    "ngrammed_corpus = [trigram_phraser[bigram_phraser[doc]] for doc in train_data]\n",
    "\n",
    "# remove stopwords from ngrammed corpus\n",
    "final_corpus = [filter_tokens(doc) for doc in ngrammed_corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences= final_corpus, vector_size= 300, window= 5, min_count= 20, workers=4)\n",
    "# Save the model\n",
    "w2v_model.save(\"models/w2v_ngram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f08af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_l = []; trigram_l = []\n",
    "for word in w2v_model.wv.key_to_index:\n",
    "    if '_' in word:\n",
    "        if len(word.split('_')) == 2:\n",
    "            bigram_l.append(word)\n",
    "        elif len(word.split('_')) == 3:\n",
    "            trigram_l.append(word)\n",
    "\n",
    "bigram_l = list(set(bigram_l)); trigram_l = list(set(trigram_l));\n",
    "#sort n-grams by frequency in descending order\n",
    "bigram_l = sorted(bigram_l, key= lambda x: w2v_model.wv.get_vecattr(x, 'count'), reverse= True)\n",
    "trigram_l = sorted(trigram_l, key= lambda x: w2v_model.wv.get_vecattr(x, 'count'), reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c408820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sandía', 0.8817235231399536),\n",
       " ('smoothie', 0.8581047654151917),\n",
       " ('melón', 0.8543225526809692),\n",
       " ('papaya', 0.8162096738815308),\n",
       " ('manzana_verde', 0.8017825484275818),\n",
       " ('kiwis', 0.797656774520874),\n",
       " ('moras', 0.7970864772796631),\n",
       " ('granada', 0.7821431756019592),\n",
       " ('lassi', 0.7561793923377991),\n",
       " ('gin', 0.7530879378318787)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('kiwi', topn= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d225e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aceite', 0.8034103512763977),\n",
       " ('oliva', 0.802460789680481),\n",
       " ('aceite_vegetal', 0.7276191115379333),\n",
       " ('girasol', 0.6712854504585266),\n",
       " ('vinagre_blanco', 0.6160853505134583),\n",
       " ('de_vinagre_balsámico', 0.6031593680381775),\n",
       " ('cebolla_picada', 0.5954839587211609),\n",
       " ('ajo_molido', 0.586854875087738),\n",
       " ('pochar', 0.582991898059845),\n",
       " ('vinagre_tinto', 0.573887825012207)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find word most similar to a sentence\n",
    "def most_similar_sentence(model, sentence, topn=10):\n",
    "    # Preprocess the sentence\n",
    "    preprocessed_sentence = preprocess_text(sentence)\n",
    "    # Tokenize the preprocessed sentence\n",
    "    tokens = preprocessed_sentence.split()\n",
    "    # Get the vector for the sentence by averaging the word vectors\n",
    "    sentence_vector = sum(model.wv[word] for word in tokens if word in model.wv) / len(tokens)\n",
    "    # Find the most similar words to the sentence vector\n",
    "    similar_words = model.wv.similar_by_vector(sentence_vector, topn=topn)\n",
    "    return similar_words\n",
    "\n",
    "most_similar_sentence(w2v_model, \"aceite de oliva\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea80606",
   "metadata": {},
   "source": [
    "### Manual Ingredient List for Logistic Regression\n",
    "\n",
    "I am manually creating a list of ingredients to build a logistic regression model that predicts whether a word or n-gram in the vocabulary is an ingredient or not. This approach is a temporary solution and will be replaced in the future by more advanced techniques such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging, once I am more familiar with those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d36b802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_ingrediente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aceite</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sal</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gramos</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agua</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasta_que</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          is_ingrediente\n",
       "aceite              True\n",
       "sal                 True\n",
       "gramos             False\n",
       "agua                True\n",
       "hasta_que          False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example ingredient list (replace with your actual list)\n",
    "ingrediente_list = ['aceite', 'sal', 'azúcar', 'harina', 'huevo', 'leche', 'tomate', 'cebolla', 'ajo', 'pollo', 'arroz', 'jamón',\n",
    "                 'pasta', 'carne', 'pimiento', 'pimienta', 'zanahoria', 'papa', 'queso', 'mantequilla', 'chocolate', 'fruta', 'verdura',\n",
    "                 'pescado', 'marisco', 'especias', 'hierbas', 'vinagre', 'salsa', 'mostaza', 'mayonesa', 'ketchup', 'vainilla',\n",
    "                 'pan', 'tortilla', 'yogur', 'gelatina', 'miel', 'azafrán', 'pimienta', 'comino', 'orégano', 'laurel', 'maíz', \n",
    "                 'canela', 'clavo', 'jengibre', 'nuez', 'agua', 'crema', 'masa', 'limón','oliva', 'cerdo', 'atún', 'jugo', 'naranja',\n",
    "                 'patata', 'champiñon', 'vino', 'calabaza', 'coco', 'avena', 'garbanzo', 'espinaca', 'sofrito', 'piña', 'cilantro',\n",
    "                 'pimentón', 'brócoli', 'almendra', 'fresa', 'coliflor', 'yema', 'chorizo', 'mermelada', 'berenjena', 'trigo', 'vegetal',\n",
    "                 'verdadur', 'bacon', 'mango', \"quinoa\", \"fideo\", \"aceituna\", \"limon\", 'pepino', 'chile', 'camaron', 'cacao', 'lenteja',\n",
    "                 'margarina', 'frijol', 'pavo', 'yuca', 'tofu', 'romero', 'perejil', 'brandy', 'alcachofa', 'camarón', 'ostión', 'soja',\n",
    "                 'soya', 'turrón', 'fécula', 'cúrcuma', 'sésamo', 'chía', 'chia', 'mozzarella', 'culantro', 'hierbabuena', 'manzana',\n",
    "                 'espirulina', 'cardamomo', 'semilla', 'hoja', 'melón', 'kiwi', 'papaya', 'freson', 'mora', 'salmón', 'pita', 'cruton']\n",
    "\n",
    "# Create DataFrame with word and is_ingrediente columns\n",
    "df_vocab = pd.DataFrame(index= list(w2v_model.wv.key_to_index.keys()), columns= ['is_ingrediente'])\n",
    "for i_idx in df_vocab.index:\n",
    "    df_vocab.loc[i_idx, 'is_ingrediente'] = any([ww in i_idx for ww in ingrediente_list])\n",
    "\n",
    "# save data\n",
    "df_vocab.dropna().to_csv('data/is_ingrediente.csv')\n",
    "df_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c2a8996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09723923490059361"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vocab['is_ingrediente'].sum()/df_vocab.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774fb197",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\AI Projects\\\\Data\\\\SBW-vectors-300-min5.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the binary Word2Vec model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAI Projects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSBW-vectors-300-min5.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[0;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[1;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[0;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\AI Projects\\\\Data\\\\SBW-vectors-300-min5.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the binary Word2Vec model\n",
    "model = KeyedVectors.load_word2vec_format(\"C:\\AI Projects\\Data\\SBW-vectors-300-min5.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fef799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
