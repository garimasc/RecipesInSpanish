{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47530785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\garim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Download Spanish stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e51fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "def remove_stopwords(text, stop_words= spanish_stopwords):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    doc = nlp(' '.join(words))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def preprocess_text(text, remove_stopwords= False, remove_stopwords_func= remove_stopwords):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        text = remove_stopwords_func(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def filter_tokens(tokens, stop_words= spanish_stopwords):\n",
    "    return [tok for tok in tokens if ('_' in tok or tok not in stop_words)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b50a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola visitas para más información cuántos años tienes tengo años\n",
      "hola visitas información cuántos años años\n",
      "holar visita información cuántos año año\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "spanish_text = \"¡Hola! Visitas https://example.com para más información. ¿Cuántos años tienes? Tengo 25 años.\"\n",
    "cleaned_text = preprocess_text(spanish_text, remove_stopwords= False)\n",
    "print(cleaned_text)\n",
    "\n",
    "cleaned_text = remove_stopwords(cleaned_text)\n",
    "print(cleaned_text)\n",
    "\n",
    "cleaned_text = \" \".join(lemmatize_words(cleaned_text.split()))\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042d2f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>url</th>\n",
       "      <th>ingredientes</th>\n",
       "      <th>pasos</th>\n",
       "      <th>pais</th>\n",
       "      <th>duracion</th>\n",
       "      <th>porciones</th>\n",
       "      <th>calorias</th>\n",
       "      <th>categoria</th>\n",
       "      <th>contexto</th>\n",
       "      <th>comensales</th>\n",
       "      <th>tiempo</th>\n",
       "      <th>dificultad</th>\n",
       "      <th>categoria 2</th>\n",
       "      <th>valoracion</th>\n",
       "      <th>votos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>berenjenas rellenas</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>2 berenjenas, 1 pimiento rojo, 1 pimiento amar...</td>\n",
       "      <td>paso 1. lava bien las berenjenas y pártelas po...</td>\n",
       "      <td>españa</td>\n",
       "      <td>45 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alto en fibra, sin grasas trans, sin sodio o s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcachofas al horno con pico de gallo</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>4 alcachofas, 1 limón, 400 g de carne picada (...</td>\n",
       "      <td>paso 1. precalentar el horno a 180?°c. , paso ...</td>\n",
       "      <td>españa</td>\n",
       "      <td>60 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, sin grasa, alto en fibra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arroz basmati salteado con heura y verduras va...</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>200 g de arroz basmati integral, 450 ml de agu...</td>\n",
       "      <td>paso 1. hervir el arroz basmati durante unos 2...</td>\n",
       "      <td>españa</td>\n",
       "      <td>40 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, alto en grasas, bueno fuente...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tataki de atún</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>200 gramos de atún rojo (fresco o congelado), ...</td>\n",
       "      <td>paso 1. cortar en forma rectangular para el ta...</td>\n",
       "      <td>españa</td>\n",
       "      <td>60 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, sin grasa, alto en fibra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>merluza al vapor a la gallega</td>\n",
       "      <td>https://www.elmueble.com/cocinas/comidas-salud...</td>\n",
       "      <td>700 g de merluza en rodajas, 3 ajos, 1 manojo ...</td>\n",
       "      <td>paso 1. pelar y lavar las patatas. retirar el ...</td>\n",
       "      <td>españa</td>\n",
       "      <td>45 min</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bajo en calorías, sin grasa, alto en fibra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              nombre  \\\n",
       "0                                berenjenas rellenas   \n",
       "1              alcachofas al horno con pico de gallo   \n",
       "2  arroz basmati salteado con heura y verduras va...   \n",
       "3                                     tataki de atún   \n",
       "4                      merluza al vapor a la gallega   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "1  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "2  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "3  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "4  https://www.elmueble.com/cocinas/comidas-salud...   \n",
       "\n",
       "                                        ingredientes  \\\n",
       "0  2 berenjenas, 1 pimiento rojo, 1 pimiento amar...   \n",
       "1  4 alcachofas, 1 limón, 400 g de carne picada (...   \n",
       "2  200 g de arroz basmati integral, 450 ml de agu...   \n",
       "3  200 gramos de atún rojo (fresco o congelado), ...   \n",
       "4  700 g de merluza en rodajas, 3 ajos, 1 manojo ...   \n",
       "\n",
       "                                               pasos    pais duracion  \\\n",
       "0  paso 1. lava bien las berenjenas y pártelas po...  españa   45 min   \n",
       "1  paso 1. precalentar el horno a 180?°c. , paso ...  españa   60 min   \n",
       "2  paso 1. hervir el arroz basmati durante unos 2...  españa   40 min   \n",
       "3  paso 1. cortar en forma rectangular para el ta...  españa   60 min   \n",
       "4  paso 1. pelar y lavar las patatas. retirar el ...  españa   45 min   \n",
       "\n",
       "  porciones  calorias categoria contexto  comensales tiempo  dificultad  \\\n",
       "0         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "1         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "2         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "3         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "4         4       NaN       NaN      NaN         NaN    NaN         NaN   \n",
       "\n",
       "                                         categoria 2  valoracion  votos  \n",
       "0  alto en fibra, sin grasas trans, sin sodio o s...         NaN    NaN  \n",
       "1         bajo en calorías, sin grasa, alto en fibra         NaN    NaN  \n",
       "2  bajo en calorías, alto en grasas, bueno fuente...         NaN    NaN  \n",
       "3         bajo en calorías, sin grasa, alto en fibra         NaN    NaN  \n",
       "4         bajo en calorías, sin grasa, alto en fibra         NaN    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/recetas_limpias.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb0321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the 'pasos' column. Dont remove stopwords\n",
    "train_data = df['pasos'].dropna().apply(preprocess_text).str.split()\n",
    "train_data = pd.concat([train_data, df['contexto'].dropna().apply(preprocess_text).str.split()], axis=0) # add context sentences\n",
    "train_data = pd.concat([train_data, df['ingredientes'].dropna().apply(preprocess_text).str.split()], axis=0) # add inhredient list\n",
    "\n",
    "# Detect frequent bigrams and trigrams. Bigrams should appear in atleast 100 recipes\n",
    "bigram = Phrases(train_data, min_count= 100, threshold= 20)\n",
    "trigram = Phrases(bigram[train_data], min_count= 25, threshold= 5)\n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Step 4: Apply the phrasers to get merged n-grams\n",
    "ngrammed_corpus = [trigram_phraser[bigram_phraser[doc]] for doc in train_data]\n",
    "\n",
    "# remove stopwords from ngrammed corpus\n",
    "final_corpus = [filter_tokens(doc) for doc in ngrammed_corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences= final_corpus, vector_size= 300, window= 5, min_count= 20, workers=4)\n",
    "# Save the model\n",
    "w2v_model.save(\"models/w2v_ngram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f08af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_l = []; trigram_l = []\n",
    "for word in w2v_model.wv.key_to_index:\n",
    "    if '_' in word:\n",
    "        if len(word.split('_')) == 2:\n",
    "            bigram_l.append(word)\n",
    "        elif len(word.split('_')) == 3:\n",
    "            trigram_l.append(word)\n",
    "\n",
    "bigram_l = list(set(bigram_l)); trigram_l = list(set(trigram_l));\n",
    "#sort n-grams by frequency in descending order\n",
    "bigram_l = sorted(bigram_l, key= lambda x: w2v_model.wv.get_vecattr(x, 'count'), reverse= True)\n",
    "trigram_l = sorted(trigram_l, key= lambda x: w2v_model.wv.get_vecattr(x, 'count'), reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c408820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sandía', 0.866736888885498),\n",
       " ('smoothie', 0.8486126661300659),\n",
       " ('melón', 0.8383522033691406),\n",
       " ('papaya', 0.8189638257026672),\n",
       " ('manzana_verde', 0.8144549131393433),\n",
       " ('moras', 0.8057365417480469),\n",
       " ('granada', 0.7657057642936707),\n",
       " ('confitada', 0.7597187161445618),\n",
       " ('coulis', 0.7574557662010193),\n",
       " ('kiwis', 0.7389394640922546)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('kiwi', topn= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d225e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oliva', 0.7995582818984985),\n",
       " ('aceite', 0.7995444536209106),\n",
       " ('aceite_vegetal', 0.7193314433097839),\n",
       " ('girasol', 0.6715099215507507),\n",
       " ('generoso', 0.6019428372383118),\n",
       " ('ajo_molido', 0.6000442504882812),\n",
       " ('vinagre_blanco', 0.5950122475624084),\n",
       " ('vino_blanco', 0.5868308544158936),\n",
       " ('ajo_picado', 0.5797868967056274),\n",
       " ('de_vinagre_balsámico', 0.5649110078811646)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find word most similar to a sentence\n",
    "def most_similar_sentence(model, sentence, topn=10):\n",
    "    # Preprocess the sentence\n",
    "    preprocessed_sentence = preprocess_text(sentence)\n",
    "    # Tokenize the preprocessed sentence\n",
    "    tokens = preprocessed_sentence.split()\n",
    "    # Get the vector for the sentence by averaging the word vectors\n",
    "    sentence_vector = sum(model.wv[word] for word in tokens if word in model.wv) / len(tokens)\n",
    "    # Find the most similar words to the sentence vector\n",
    "    similar_words = model.wv.similar_by_vector(sentence_vector, topn=topn)\n",
    "    return similar_words\n",
    "\n",
    "most_similar_sentence(w2v_model, \"aceite de oliva\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40557d",
   "metadata": {},
   "source": [
    "### Fine-Tune Spanish Billion Words (SBW) Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7dd5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in thread Thread-249 (_worker_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "Thread-248 (_worker_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "Exception in thread Thread-250 (_worker_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    self.run()\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    self.run()\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\threading.py\", line 953, in run\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\threading.py\", line 953, in run\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 1166, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 1166, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 1166, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 957, in _do_train_job\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 957, in _do_train_job\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"c:\\Users\\garim\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 957, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim\\models\\word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim\\models\\word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim\\models\\word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "  File \"gensim\\models\\word2vec_inner.pyx\", line 475, in gensim.models.word2vec_inner.init_w2v_config\n",
      "  File \"gensim\\models\\word2vec_inner.pyx\", line 475, in gensim.models.word2vec_inner.init_w2v_config\n",
      "  File \"gensim\\models\\word2vec_inner.pyx\", line 475, in gensim.models.word2vec_inner.init_w2v_config\n",
      "AttributeError: 'KeyedVectors' object has no attribute 'vectors_lockf'. Did you mean: 'vectors_norm'?\n",
      "AttributeErrorAttributeError: 'KeyedVectors' object has no attribute 'vectors_lockf'. Did you mean: 'vectors_norm'?\n",
      ": 'KeyedVectors' object has no attribute 'vectors_lockf'. Did you mean: 'vectors_norm'?\n"
     ]
    }
   ],
   "source": [
    "# Load the binary Spanish Billion Words(SBW) Word2Vec model\n",
    "sbw_w2v = KeyedVectors.load_word2vec_format(\"C:\\AI Projects\\Data\\SBW-vectors-300-min5.bin\\sbw_vectors.bin\", binary=True)\n",
    "\n",
    "# Convert KeyedVectors to full Word2Vec (hacky)\n",
    "sbw_finetuned_w2v_ngram = Word2Vec(vector_size=300, min_count=1)\n",
    "\n",
    "sbw_finetuned_w2v_ngram.build_vocab([list(sbw_w2v.key_to_index.keys())], update=False)\n",
    "sbw_finetuned_w2v_ngram.wv.vectors = sbw_w2v.vectors\n",
    "sbw_finetuned_w2v_ngram.wv.key_to_index = sbw_w2v.key_to_index\n",
    "sbw_finetuned_w2v_ngram.wv.index_to_key = sbw_w2v.index_to_key\n",
    "\n",
    "# # Update vocabulary and retrain\n",
    "sbw_finetuned_w2v_ngram.build_vocab(final_corpus, update=True)\n",
    "sbw_finetuned_w2v_ngram.train(final_corpus, total_examples=len(final_corpus), epochs=5)\n",
    "\n",
    "sbw_finetuned_w2v_ngram.save(\"models/sbw_finetuned_w2v_ngram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea80606",
   "metadata": {},
   "source": [
    "### Manual Ingredient List for Logistic Regression\n",
    "\n",
    "I am manually creating a list of ingredients to build a logistic regression model that predicts whether a word or n-gram in the vocabulary is an ingredient or not. This approach is a temporary solution and will be replaced in the future by more advanced techniques such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging, once I am more familiar with those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d36b802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_ingrediente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aceite</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sal</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gramos</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agua</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasta_que</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          is_ingrediente\n",
       "aceite              True\n",
       "sal                 True\n",
       "gramos             False\n",
       "agua                True\n",
       "hasta_que          False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example ingredient list (replace with your actual list)\n",
    "ingrediente_list = ['aceite', 'sal', 'azúcar', 'harina', 'huevo', 'leche', 'tomate', 'cebolla', 'ajo', 'pollo', 'arroz', 'jamón',\n",
    "                 'pasta', 'carne', 'pimiento', 'pimienta', 'zanahoria', 'papa', 'queso', 'mantequilla', 'chocolate', 'fruta', 'verdura',\n",
    "                 'pescado', 'marisco', 'especias', 'hierbas', 'vinagre', 'salsa', 'mostaza', 'mayonesa', 'ketchup', 'vainilla',\n",
    "                 'pan', 'tortilla', 'yogur', 'gelatina', 'miel', 'azafrán', 'pimienta', 'comino', 'orégano', 'laurel', 'maíz', \n",
    "                 'canela', 'clavo', 'jengibre', 'nuez', 'agua', 'crema', 'masa', 'limón','oliva', 'cerdo', 'atún', 'jugo', 'naranja',\n",
    "                 'patata', 'champiñon', 'vino', 'calabaza', 'coco', 'avena', 'garbanzo', 'espinaca', 'sofrito', 'piña', 'cilantro',\n",
    "                 'pimentón', 'brócoli', 'almendra', 'fresa', 'coliflor', 'yema', 'chorizo', 'mermelada', 'berenjena', 'trigo', 'vegetal',\n",
    "                 'verdadur', 'bacon', 'mango', \"quinoa\", \"fideo\", \"aceituna\", \"limon\", 'pepino', 'chile', 'camaron', 'cacao', 'lenteja',\n",
    "                 'margarina', 'frijol', 'pavo', 'yuca', 'tofu', 'romero', 'perejil', 'brandy', 'alcachofa', 'camarón', 'ostión', 'soja',\n",
    "                 'soya', 'turrón', 'fécula', 'cúrcuma', 'sésamo', 'chía', 'chia', 'mozzarella', 'culantro', 'hierbabuena', 'manzana',\n",
    "                 'espirulina', 'cardamomo', 'semilla', 'hoja', 'melón', 'kiwi', 'papaya', 'freson', 'mora', 'salmón', 'pita', 'cruton']\n",
    "\n",
    "# Create DataFrame with word and is_ingrediente columns\n",
    "df_vocab = pd.DataFrame(index= list(w2v_model.wv.key_to_index.keys()), columns= ['is_ingrediente'])\n",
    "for i_idx in df_vocab.index:\n",
    "    df_vocab.loc[i_idx, 'is_ingrediente'] = any([ww in i_idx for ww in ingrediente_list])\n",
    "\n",
    "# save data\n",
    "df_vocab.dropna().to_csv('data/is_ingrediente.csv')\n",
    "df_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2a8996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09723923490059361"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vocab['is_ingrediente'].sum()/df_vocab.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61fef799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['azucar'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c98466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
